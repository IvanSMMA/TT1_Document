\chapter{Resultados}
\section{Resultados del Modulo de PLN}

% \subsection{Métricas de Precisión}

% El sistema fue evaluado utilizando métricas clásicas de desempeño para modelos de recuperación semántica y detección basada en similitud.

% \noindent\textbf{Tasa de éxito: 92.3\%}
% \begin{itemize}
%     \item Tests aprobados: 191/204
%     \item Interpretación: el sistema funciona correctamente en más del 90\% de los casos evaluados.
% \end{itemize}

% \noindent\textbf{Precisión: 92.3\%}
% \[
% \text{Precision} = \frac{TP}{TP + FP}
% \]
% \begin{itemize}
%     \item Indica que el 92.3\% de las respuestas entregadas por el sistema son correctas.
% \end{itemize}

% \noindent\textbf{Recall: $\sim$88\%}
% \[
% \text{Recall} = \frac{TP}{TP + FN}
% \]
% \begin{itemize}
%     \item Representa que el 88\% de los casos que debían ser detectados fueron efectivamente recuperados.
% \end{itemize}

% \noindent\textbf{F1-Score: $\sim$90\%}
% \[
% F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
% \]
% \begin{itemize}
%     \item Mide el equilibrio entre precisión y recall, con un rendimiento general cercano al 90\%.
% \end{itemize}


% \subsection{Métricas de Rendimiento}

% \noindent\textbf{Latencia P50: 42ms}
% \begin{itemize}
%     \item El 50\% de las solicitudes concluyen en $\leq 42$ms.
%     \item Objetivo: $<$50ms.
%     \item Estado: \textbf{Excelente}.
% \end{itemize}

% \noindent\textbf{Latencia P95: 50ms}
% \begin{itemize}
%     \item El 95\% de las solicitudes concluyen en $\leq 50$ms.
%     \item Objetivo: $<$100ms.
%     \item Estado: \textbf{Excelente}.
% \end{itemize}

% \noindent\textbf{Throughput: 25 qps}
% \begin{itemize}
%     \item Consultas por segundo en una sola instancia.
%     \item Objetivo: $>$20 qps.
%     \item Estado: \textbf{Bueno}.
% \end{itemize}

% \noindent\textbf{Inicialización: 1.37ms}
% \begin{itemize}
%     \item Valor logrado utilizando caché de embeddings.
%     \item Sin caché: $\sim$2000ms.
%     \item Estado: \textbf{Óptimo}.
% \end{itemize}


% \subsection{Métricas de Usabilidad}

% \noindent\textbf{SUS Score proyectado: 75}
% \begin{itemize}
%     \item Escala: 0--100.
%     \item Interpretación: Grado B (Calificación ``buena'').
%     \item Benchmark: 68 corresponde al promedio general.
% \end{itemize}

% \noindent\textbf{NPS proyectado: +35}
% \begin{itemize}
%     \item Escala: $-100$ a $+100$.
%     \item Interpretación: Valor considerado ``muy bueno''.
%     \item Benchmark: 0 se considera neutral.
% \end{itemize}

% \noindent\textbf{CSAT proyectado: 5.8/7}
% \begin{itemize}
%     \item Escala: 1--7.
%     \item Interpretación: Nivel de satisfacción alto.
%     \item Benchmark: 5 se considera aceptable.
% \end{itemize}


% \subsection{Comparación con Alternativas}

% \noindent\textbf{Alternativa 1: String Matching (regex, fuzzy)}
% \begin{itemize}
%     \item Precisión: $\sim$40\%
%     \item Latencia: $\sim$1ms
%     \item Conclusión: Muy rápido, pero altamente impreciso.
% \end{itemize}

% \noindent\textbf{Alternativa 2: Word2Vec + Cosine Similarity}
% \begin{itemize}
%     \item Precisión: $\sim$60\%
%     \item Latencia: $\sim$20ms
%     \item Conclusión: Mejor que regex, pero muy inferior a Transformers modernos.
% \end{itemize}

% \noindent\textbf{Alternativa 3: BERT-large}
% \begin{itemize}
%     \item Precisión: $\sim$96\%
%     \item Latencia: $\sim$120ms
%     \item Conclusión: Altamente preciso, pero ineficiente para dispositivos móviles o consultas en tiempo real.
% \end{itemize}

% \noindent\textbf{Modelo de este proyecto: Sentence-BERT MiniLM-L12}
% \begin{itemize}
%     \item Precisión: $\sim$92\%
%     \item Latencia: $\sim$42ms
%     \item Conclusión: \textbf{Óptimo} en balance precisión/velocidad.
% \end{itemize}

\textbf{Conclusión}: Todos los objetivos fueron superados significativamente.\\

\noindent \textbf{Fortalezas}

\begin{enumerate}
    \item Cobertura exhaustiva
    \begin{itemize}[label=\checkmark]
        \item 168 tests implementados en todos los niveles
        \item 90.8\% de cobertura supera estándares industriales (80\%)
        \item Distribución apropiada según pirámide de testing
    \end{itemize}

    \item Robustez lingüística mínima aceptable
    \begin{itemize}[label=\checkmark]
        \item 50+ casos de perturbaciones validados
        \item >95\% de tolerancia a errores ortográficos
        \item Degradación gradual y graceful
        \item Normalización efectiva de leet speak
    \end{itemize}

    \item Calidad semántica aceptable
    \begin{itemize}[label=\checkmark]
        \item F1-Score de 88\% demuestra balance precision/recall
        \item 100\% de golden dataset passing
        \item Reconocimiento de sinónimos >75\%
    \end{itemize}

    \item Rendimiento muy adecuado
    \begin{itemize}[label=\checkmark]
        \item Latencia P95 de 68ms (66\% mejor que objetivo)
        \item Success rate de 99.7\% bajo carga
        \item Sistema estable hasta 100 usuarios concurrentes
    \end{itemize}

    \item Validación de casos críticos
    \begin{itemize}[label=\checkmark]
        \item Similitud matemáticamente correcta [0.0, 1.0] en 100\% de casos
        \item Caso edge ``Ivan'' correctamente manejado
        \item Detección de patrones ``Me llamo [NOMBRE]''
        \item Manejo robusto de nombres propios
    \end{itemize}
\end{enumerate}

{\large \noindent \textbf{Optimizaciones y rendimiento}}\\

\textbf{Caché de embeddings}\\

\textbf{Problema}: Generar embeddings es costoso:
\begin{itemize}
    \item Tiempo: $\sim$5 segundos para 43 frases
    \item Requiere modelo cargado en memoria
    \item Se ejecuta en cada startup
\end{itemize}

\textbf{Solución}: Caché persistente en archivo .npz:
\begin{itemize}
    \item Formato: NumPy compressed (.npz)
    \item Tamaño: $\sim$50 KB comprimido
    \item Carga: $<1$ segundo
\end{itemize}

\vspace{1em}

\textbf{Búsqueda jerárquica por centroides}\\

\textbf{Problema}: Búsqueda exhaustiva en 43 frases:
\begin{itemize}
    \item O(N) comparaciones con N = 43
    \item No escala bien con más frases
    \item Ineficiente para datasets grandes
\end{itemize}

\textbf{Solución}: Búsqueda jerárquica en dos fases:
\begin{enumerate}
    \item Fase 1: Buscar top-3 grupos (O(K) con K=3)
    \item Fase 2: Buscar en grupos candidatos (O(N\_k))
\end{enumerate}

Complejidad: O(K + N\_k) $\ll$ O(N)\\

\textbf{Resultados}: Dataset actual (43 frases):\\

{\large \noindent \textbf{Optimización de latencia}}\\
\textbf{Técnicas aplicadas}\\

\begin{enumerate}

    \item \textbf{Operaciones vectorizadas (NumPy)}
    \\[4pt]
    \begin{verbatim}
# LENTO: Loop
for i, emb in enumerate(embeddings):
    sim[i] = cosine_similarity([query_emb], [emb])

# RÁPIDO: Vectorizado
sims = cosine_similarity([query_emb], embeddings)[0]
    \end{verbatim}
    Speedup: $\sim$100x\\[10pt]

    \item \textbf{Lazy loading del modelo}
    \\[4pt]
    \begin{verbatim}
def _load_model(self):
    if self.model is None:
        self.model = SentenceTransformer(model_name)
    \end{verbatim}
    Ahorro: No cargar modelo si no hay requests\\[10pt]

    \item \textbf{Batch processing}
    \\[4pt]
    \begin{verbatim}
embeddings = model.encode(texts, batch_size=32)
    \end{verbatim}
    Speedup: $\sim$2x para múltiples textos\\[10pt]

    \item \textbf{Async I/O (FastAPI)}
    \\[4pt]
    \begin{verbatim}
@app.post("/buscar")
async def buscar(request: QueryRequest):
    ...
    \end{verbatim}
    Permite manejar múltiples requests concurrentes\\[6pt]

\end{enumerate}

{\large \noindent \textbf{Profile de latencia}}\\
\textbf{Total latency}: 40ms\\

Breakdown:
\begin{itemize}
    \item Preprocesamiento: 2ms (5\%)
    \item Generación de embedding: 15ms (37.5\%)
    \item Búsqueda por centroides: 3ms (7.5\%)
    \item Re-ranking: 10ms (25\%)
    \item Detección de patrones: 5ms (12.5\%)
    \item Construcción de respuesta: 3ms (7.5\%)
    \item Overhead (FastAPI): 2ms (5\%)
\end{itemize}

\vspace{1em}

\textbf{Cuellos de botella}:
\begin{enumerate}
    \item Generación de embedding (37.5\%) $\leftarrow$ Mayor oportunidad
    \item Re-ranking (25\%)
\end{enumerate}

\vspace{1em}

\textbf{Optimizaciones}
\begin{enumerate}
    \item Modelo ligero (MiniLM)
    \begin{itemize}
        \item 384 dimensiones vs 768 (BERT base)
        \item 420 MB vs 800 MB
        \item Ahorro: 47.5\%
    \end{itemize}

    \item Embeddings comprimidos
    \begin{itemize}
        \item Formato .npz comprimido
        \item 50 KB vs $\sim$100 KB sin comprimir
        \item Ahorro: 50\%
    \end{itemize}

    \item Garbage collection optimizado
    \begin{verbatim}
    import gc
    gc.collect()  # Después de carga inicial
    \end{verbatim}
\end{enumerate}

{\large \noindent \textbf{Escalabilidad}}\\
\textbf{Escalabilidad horizontal}\\
API stateless permite múltiples instancias:\\

\textbf{Configuración}:
\begin{verbatim}
# Instancia 1
uvicorn app.main:app --port 8001 &

# Instancia 2
uvicorn app.main:app --port 8002 &

# Instancia 3
uvicorn app.main:app --port 8003 &

# Load balancer (nginx)
upstream api_servers {
    server localhost:8001;
    server localhost:8002;
    server localhost:8003;
}
\end{verbatim}

\noindent \textbf{Escalabilidad con workers}:
\begin{verbatim}
uvicorn app.main:app \
    --host 0.0.0.0 \
    --port 8000 \
    --workers 4
\end{verbatim}

\textbf{Throughput}:
\begin{itemize}
    \item 1 worker: $\sim$25 req/s
    \item 4 workers: $\sim$90 req/s (3.6x)
\end{itemize}

\noindent \textbf{Limitaciones}
\begin{itemize}
    \item Modelo en memoria: $\sim$420 MB por worker
    \item 4 workers: $\sim$1.7 GB memoria
    \item Máximo recomendado: 8 workers en servidor 8GB RAM
\end{itemize}

\vspace{1em}

{\Large \noindent \textbf{Resultados y métricas}}\\
{\large \textbf{Métricas de precisión}}\\

\noindent \textbf{Dataset de evaluación}\\
100 queries de prueba en 3 categorías:
\begin{itemize}
    \item 40 queries de emergencias (Grupo A)
    \item 30 queries de saludos (Grupo B)
    \item 30 queries de comunicación (Grupo C)
\end{itemize}

Matriz de confusión\\

Observaciones:
\begin{itemize}
    \item Grupo A (Emergencias): Mejor precisión (95\%)
    \item Confusión menor entre grupos similares
    \item Errores en casos ambiguos
\end{itemize}

{\large \noindent \textbf{Métricas de usabilidad}}\\

\textbf{Swagger UI}
\begin{itemize}[label= \checkmark]
    \item Documentación automática
    \item Testing interactivo
    \item Ejemplos de requests
    \item Schemas completos
    \item Respuestas de error documentadas
\end{itemize}

\textbf{API Consistency}
\begin{itemize}[label= \checkmark]
    \item Formato JSON consistente
    \item Campos opcionales claramente marcados
    \item Validación automática de requests
    \item Mensajes de error descriptivos
    \item HTTP status codes apropiados
\end{itemize}

\vspace{1em}

{\large \noindent \textbf{Comparación con alternativas}}\\

\textbf{Ventajas del prototipo}
\begin{itemize}[label= \checkmark]
    \item Mayor precisión (92\% vs 60--75\%)
    \item Manejo de paráfrasis
    \item Detección de nombres propios
    \item Sistema de deletreo automático
    \item API bien documentada
    \item Tests exhaustivos
    \item Fácil de escalar
\end{itemize}

\textbf{Desventajas}
\begin{itemize}
    \item Mayor latencia (40ms vs 5--25ms)
    \item Mayor uso de memoria ($\sim$500MB vs 50--200MB)
    \item Requiere modelo pre-entrenado
\end{itemize}
